{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDF Conversion Workflow Demo\n",
    "\n",
    "This notebook demonstrates converting PMC XML articles to RDF for integration with knowledge graphs.\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "- **Semantic Integration**: Integrate scientific literature with knowledge graphs\n",
    "- **SPARQL Queries**: Query literature data using SPARQL\n",
    "- **Ontology Alignment**: Align to standard ontologies (BIBO, FOAF, DCT)\n",
    "- **GraphDB Loading**: Prepare data for triple stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from pyeuropepmc.processing.fulltext_parser import FullTextXMLParser\n",
    "from pyeuropepmc.builders import build_paper_entities\n",
    "from pyeuropepmc.mappers import RDFMapper\n",
    "from rdflib import Graph, Namespace\n",
    "from rdflib.namespace import RDF, RDFS\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Parse a PMC XML File\n",
    "\n",
    "Let's start by loading a real PMC article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a fixture file\n",
    "fixture_path = \"../tests/fixtures/fulltext_downloads/PMC3359999.xml\"\n",
    "\n",
    "if os.path.exists(fixture_path):\n",
    "    with open(fixture_path, 'r') as f:\n",
    "        xml_content = f.read()\n",
    "    print(f\"Loaded XML file: {len(xml_content)} characters\")\n",
    "else:\n",
    "    print(f\"File not found: {fixture_path}\")\n",
    "    print(\"Please adjust the path to point to a PMC XML file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the XML\n",
    "parser = FullTextXMLParser(xml_content)\n",
    "\n",
    "# Build entities\n",
    "paper, authors, sections, tables, references = build_paper_entities(parser)\n",
    "\n",
    "print(f\"Paper: {paper.title}\")\n",
    "print(f\"PMCID: {paper.pmcid}\")\n",
    "print(f\"DOI: {paper.doi}\")\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"  Authors: {len(authors)}\")\n",
    "print(f\"  Sections: {len(sections)}\")\n",
    "print(f\"  Tables: {len(tables)}\")\n",
    "print(f\"  References: {len(references)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Normalize and Validate Entities\n",
    "\n",
    "Before converting to RDF, normalize and validate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DOI before normalization\n",
    "print(f\"DOI before normalization: {paper.doi}\")\n",
    "\n",
    "# Normalize paper\n",
    "paper.normalize()\n",
    "print(f\"DOI after normalization: {paper.doi}\")\n",
    "\n",
    "# Validate paper\n",
    "try:\n",
    "    paper.validate()\n",
    "    print(\"✓ Paper validation passed\")\n",
    "except ValueError as e:\n",
    "    print(f\"✗ Paper validation failed: {e}\")\n",
    "\n",
    "# Normalize and validate all authors\n",
    "for author in authors:\n",
    "    author.normalize()\n",
    "    try:\n",
    "        author.validate()\n",
    "    except ValueError as e:\n",
    "        print(f\"✗ Author validation failed: {e}\")\n",
    "\n",
    "print(f\"✓ All {len(authors)} authors validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert to RDF\n",
    "\n",
    "Now convert the entities to RDF triples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RDF mapper and graph\n",
    "mapper = RDFMapper()\n",
    "g = Graph()\n",
    "\n",
    "# Bind namespaces for prettier output\n",
    "g.bind(\"dct\", Namespace(\"http://purl.org/dc/terms/\"))\n",
    "g.bind(\"bibo\", Namespace(\"http://purl.org/ontology/bibo/\"))\n",
    "g.bind(\"foaf\", Namespace(\"http://xmlns.com/foaf/0.1/\"))\n",
    "g.bind(\"prov\", Namespace(\"http://www.w3.org/ns/prov#\"))\n",
    "g.bind(\"nif\", Namespace(\"http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#\"))\n",
    "\n",
    "# Add paper to graph\n",
    "paper_uri = paper.to_rdf(g, mapper=mapper)\n",
    "print(f\"Paper URI: {paper_uri}\")\n",
    "print(f\"Triples after adding paper: {len(g)}\")\n",
    "\n",
    "# Add authors\n",
    "for i, author in enumerate(authors):\n",
    "    author_uri = author.to_rdf(g, mapper=mapper)\n",
    "    if i == 0:\n",
    "        print(f\"\\nFirst author URI: {author_uri}\")\n",
    "\n",
    "print(f\"Triples after adding {len(authors)} authors: {len(g)}\")\n",
    "\n",
    "# Add sections\n",
    "for section in sections[:3]:  # Add first 3 sections for demo\n",
    "    section.to_rdf(g, mapper=mapper)\n",
    "\n",
    "print(f\"Triples after adding 3 sections: {len(g)}\")\n",
    "\n",
    "# Add tables\n",
    "for table in tables:\n",
    "    table.to_rdf(g, mapper=mapper)\n",
    "\n",
    "print(f\"Triples after adding {len(tables)} tables: {len(g)}\")\n",
    "\n",
    "# Add references\n",
    "for reference in references[:5]:  # Add first 5 references for demo\n",
    "    reference.to_rdf(g, mapper=mapper)\n",
    "\n",
    "print(f\"\\nTotal triples in graph: {len(g)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Serialize to Turtle Format\n",
    "\n",
    "Let's view the RDF in Turtle format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize to Turtle\n",
    "ttl = mapper.serialize_graph(g, format=\"turtle\")\n",
    "\n",
    "# Display first 1000 characters\n",
    "print(\"RDF/Turtle Output (first 1000 characters):\")\n",
    "print(\"=\" * 60)\n",
    "print(ttl[:1000])\n",
    "print(\"...\")\n",
    "print(f\"\\nTotal output length: {len(ttl)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Query the RDF Graph with SPARQL\n",
    "\n",
    "Now we can query the graph using SPARQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1: Get paper metadata\n",
    "query1 = \"\"\"\n",
    "PREFIX bibo: <http://purl.org/ontology/bibo/>\n",
    "PREFIX dct: <http://purl.org/dc/terms/>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "SELECT ?paper ?title ?doi ?journal\n",
    "WHERE {\n",
    "    ?paper a bibo:AcademicArticle .\n",
    "    ?paper dct:title ?title .\n",
    "    OPTIONAL { ?paper bibo:doi ?doi }\n",
    "    OPTIONAL { ?paper bibo:journal ?journal }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Query 1: Paper Metadata\")\n",
    "print(\"=\" * 60)\n",
    "results = g.query(query1)\n",
    "for row in results:\n",
    "    print(f\"Paper: {row.paper}\")\n",
    "    print(f\"Title: {row.title}\")\n",
    "    print(f\"DOI: {row.doi}\")\n",
    "    print(f\"Journal: {row.journal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2: Get all authors\n",
    "query2 = \"\"\"\n",
    "PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "SELECT ?author ?name\n",
    "WHERE {\n",
    "    ?author a foaf:Person .\n",
    "    ?author foaf:name ?name .\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nQuery 2: Authors\")\n",
    "print(\"=\" * 60)\n",
    "results = g.query(query2)\n",
    "for i, row in enumerate(results, 1):\n",
    "    print(f\"{i}. {row.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3: Get document sections\n",
    "query3 = \"\"\"\n",
    "PREFIX bibo: <http://purl.org/ontology/bibo/>\n",
    "PREFIX dct: <http://purl.org/dc/terms/>\n",
    "PREFIX nif: <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#>\n",
    "\n",
    "SELECT ?section ?title ?content\n",
    "WHERE {\n",
    "    ?section a bibo:DocumentPart .\n",
    "    ?section dct:title ?title .\n",
    "    OPTIONAL { ?section nif:isString ?content }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nQuery 3: Document Sections\")\n",
    "print(\"=\" * 60)\n",
    "results = g.query(query3)\n",
    "for i, row in enumerate(results, 1):\n",
    "    content_preview = str(row.content)[:100] if row.content else \"No content\"\n",
    "    print(f\"{i}. {row.title}\")\n",
    "    print(f\"   Content preview: {content_preview}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export to Different RDF Formats\n",
    "\n",
    "RDF can be serialized to various formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to different formats\n",
    "formats = [\"turtle\", \"xml\", \"json-ld\"]\n",
    "\n",
    "for fmt in formats:\n",
    "    output = mapper.serialize_graph(g, format=fmt)\n",
    "    print(f\"{fmt.upper()} format: {len(output)} characters\")\n",
    "    print(f\"First 200 characters:\")\n",
    "    print(output[:200])\n",
    "    print(\"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save to Files\n",
    "\n",
    "Save the RDF to files for loading into a triple store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to files\n",
    "output_dir = \"/tmp/rdf_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save as Turtle\n",
    "ttl_path = f\"{output_dir}/paper_{paper.pmcid}.ttl\"\n",
    "mapper.serialize_graph(g, format=\"turtle\", destination=ttl_path)\n",
    "print(f\"Saved Turtle: {ttl_path}\")\n",
    "\n",
    "# Save as JSON-LD\n",
    "jsonld_path = f\"{output_dir}/paper_{paper.pmcid}.jsonld\"\n",
    "mapper.serialize_graph(g, format=\"json-ld\", destination=jsonld_path)\n",
    "print(f\"Saved JSON-LD: {jsonld_path}\")\n",
    "\n",
    "# Save as RDF/XML\n",
    "xml_path = f\"{output_dir}/paper_{paper.pmcid}.rdf\"\n",
    "mapper.serialize_graph(g, format=\"xml\", destination=xml_path)\n",
    "print(f\"Saved RDF/XML: {xml_path}\")\n",
    "\n",
    "print(f\"\\nAll files saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Graph Statistics\n",
    "\n",
    "Let's analyze the generated RDF graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count entities by type\n",
    "from collections import Counter\n",
    "\n",
    "# Query to get all types\n",
    "type_query = \"\"\"\n",
    "SELECT ?type (COUNT(?entity) as ?count)\n",
    "WHERE {\n",
    "    ?entity a ?type .\n",
    "}\n",
    "GROUP BY ?type\n",
    "ORDER BY DESC(?count)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Entity Type Distribution:\")\n",
    "print(\"=\" * 60)\n",
    "results = g.query(type_query)\n",
    "for row in results:\n",
    "    type_name = str(row.type).split(\"/\")[-1].split(\"#\")[-1]\n",
    "    print(f\"{type_name}: {row.count}\")\n",
    "\n",
    "print(f\"\\nTotal triples: {len(g)}\")\n",
    "print(f\"Total unique subjects: {len(set(g.subjects()))}\")\n",
    "print(f\"Total unique predicates: {len(set(g.predicates()))}\")\n",
    "print(f\"Total unique objects: {len(set(g.objects()))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Using the CLI Tool\n",
    "\n",
    "For batch processing, use the CLI tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example CLI usage (run in terminal)\n",
    "print(\"\"\"\n",
    "Command-line usage:\n",
    "\n",
    "# Convert single file to Turtle and JSON\n",
    "python scripts/xml_to_rdf.py input.xml --ttl output.ttl --json output.json -v\n",
    "\n",
    "# Convert multiple files\n",
    "for file in *.xml; do\n",
    "    python scripts/xml_to_rdf.py \"$file\" --ttl \"${file%.xml}.ttl\" -v\n",
    "done\n",
    "\n",
    "# Use custom RDF mapping configuration\n",
    "python scripts/xml_to_rdf.py input.xml --ttl output.ttl --config custom_map.yml\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. Loading and parsing PMC XML files\n",
    "2. Building typed entity models\n",
    "3. Normalizing and validating data\n",
    "4. Converting to RDF with ontology alignment\n",
    "5. Querying RDF with SPARQL\n",
    "6. Exporting to multiple RDF formats\n",
    "7. Saving for triple store integration\n",
    "8. Analyzing graph statistics\n",
    "9. Using the CLI tool for batch processing\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Load RDF into GraphDB, Blazegraph, or other triple stores\n",
    "- Validate with SHACL shapes (see `shacl/pub.shacl.ttl`)\n",
    "- Extend ontology mappings in `conf/rdf_map.yml`\n",
    "- Build federated queries across multiple papers\n",
    "- Integrate with existing knowledge graphs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
